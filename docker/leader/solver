#!/usr/bin/env python3
import json
import logging
import os
import subprocess
import sys
import threading
import glob
import time
from threading import Timer

import signal
from subprocess import Popen
from subprocess import TimeoutExpired
from typing import Callable
from enum import Enum


#######################################################

class PipelineMode(Enum):
    # Single machine with 64 hwthreads, sequential proof production and DRAT proof checking
    SINGLE_MACHINE_64HWT_SEQ_PPROD_AND_DRAT = 1
    # Single machine with 64 hwthreads, parallel proof production
    SINGLE_MACHINE_64HWT_PAR_PPROD = 2
    # 100 machines with 16 hwthreads, parallel proof production
    HUNDRED_MACHINES_16HWT_PAR_PPROD = 3

# TODO Change this to the desired mode of execution.
pipeline_mode = PipelineMode.SINGLE_MACHINE_64HWT_PAR_PPROD

#######################################################


logging.basicConfig(format='%(asctime)s - %(name)s - %(levelname)s - %(message)s')

def reverse_function(x):
    return x[::-1]

# total number of *local* slots (MPI processes) on this machine
def get_num_local_slots():
    #return 2 # only for testing!!
    if pipeline_mode == PipelineMode.HUNDRED_MACHINES_16HWT_PAR_PPROD:
        return 4 # distributed setup: 16/4 = 4 slots
    else:
        return 16 # shared-memory setup: 64/4 = 16 slots

# total time to run in seconds.  This allows ~1 minute of gracetime before ATS would pull the plug.
TOTAL_TIME = 4940

# Class to help identify when a timeout has been triggered
class _TimeoutRecorder:
    __timed_out: bool = False

    def record_timeout(self):
        self.__timed_out = True

    def is_timed_out(self) -> bool:
        return self.__timed_out

class Runner:
    def __init__(self, request_directory: str):
        self.logger = logging.getLogger("Runner")
        self.logger.setLevel(logging.INFO)
        self.request_directory = request_directory
        os.environ['PYTHONUNBUFFERED'] = "1"
        self.sigterm_grace_seconds = 10

    def killpg_if_running(self, process: Popen, sigkill_grace_seconds: int, post_kill_hook: Callable[[], None] = None) -> None:
        self.__sendpg_signal(process, signal.SIGTERM, post_kill_hook)
        try:
            process.wait(sigkill_grace_seconds)
        except TimeoutExpired:
            self.__sendpg_signal(process, signal.SIGKILL)


    def __sendpg_signal(self, proc: Popen, proc_signal: signal.Signals, post_kill_hook: Callable[[], None] = None) -> None:
        try:
            pg_id = os.getpgid(proc.pid)
            self.logger.info("Sending %s to process group %d for process %d", proc_signal, pg_id, proc.pid)
            os.killpg(pg_id, proc_signal)
            if post_kill_hook:
                post_kill_hook()
        except ProcessLookupError:
            # Process not found, so we can assume it has already ended. This can happen because of
            # race-conditions around when the process finishes, this method is called, and timeout
            # cancelled
            self.logger.debug("No process group for process %d found. Ignoring %s.", proc.pid, proc_signal)

    def process_stream(self, stream, str_name, file_handle):
        line = stream.readline()
        while line != "":
            self.logger.info(f"{str_name}: {line}")
            file_handle.write(line)
            line = stream.readline()

    def run(self, cmd: list, timeout_seconds, is_append=False):
        self.logger.info("Running command: %s", str(cmd))
        
        stdout_target_loc = os.path.join(self.request_directory, "stdout.log")
        stderr_target_loc = os.path.join(self.request_directory, "stderr.log")
        
        access = "a" if is_append else "w"

        with open(stdout_target_loc, access) as stdout_handle:
            with open(stderr_target_loc, access) as stderr_handle:
                start = time.monotonic()
                proc = subprocess.Popen(cmd, stdout=subprocess.PIPE, stderr=subprocess.PIPE,
                    universal_newlines=True, start_new_session=True)
                stdout_t = threading.Thread(target = self.process_stream, args=(proc.stdout, "STDOUT", stdout_handle))
                stderr_t = threading.Thread(target = self.process_stream, args=(proc.stderr, "STDERR", stderr_handle))
                stdout_t.start()
                stderr_t.start()
                timeout_recorder = _TimeoutRecorder()
                timeout_timer = Timer(
                    timeout_seconds + self.sigterm_grace_seconds,
                    lambda: self.killpg_if_running(process=proc,
                                                        # Arbitrarily picked to fail fast
                                                        sigkill_grace_seconds=3,
                                                        post_kill_hook=timeout_recorder.record_timeout))
                try: 
                    timeout_timer.start()
                    return_code = proc.wait()
                except Exception as e:
                    self.logger.info(f"Exception occurred during job run: {e}")
                    if not return_code:
                        return_code = -1
                finally:
                    timeout_timer.cancel()
                    stdout_t.join()
                    stderr_t.join()

                stop = time.monotonic()
                timing = stop-start
                timing_str = f">>>>>{cmd[0]} : {timing} seconds<<<<<\n"
                self.logger.info(f"STDERR: {timing_str}")
                stderr_handle.write(timing_str)
                
        return {
            "stdout": stdout_target_loc,
            "stderr": stderr_target_loc,
            "return_code": return_code,
            "time": timing,
            "timed_out": timeout_recorder.is_timed_out(),
            "output_directory": self.request_directory
        }


    def get_input_json(self):
        input = os.path.join(self.request_directory, "input.json")
        with open(input) as f:
            return json.loads(f.read())
    
    def create_hostfile(self,ips, request_dir):
        hostfile_path = os.path.join(request_dir, 'combined_hostfile')
        with open(hostfile_path, 'w+') as f:
            for ip in ips:
                f.write(f'{ip} slots={get_num_local_slots()}')
                f.write('\n')
            return hostfile_path

    def get_command(self, input_json):
        problem_path = input_json.get("problem_path")
        worker_node_ips = input_json.get("worker_node_ips", [])
        
        combined_hostfile = self.create_hostfile(worker_node_ips, self.request_directory)

        run_list = ["/competition/run_mallob.sh"]
        run_list.append(combined_hostfile)
        run_list.append(problem_path)
        run_list.append(pipeline_mode.name) # forward mode of execution to mallob script

        return run_list

    def get_wc_command(self):
        args = ['/competition/proof_line_count.sh']
        return args

    def get_compress_preprocessing_proof_command(self, input_json):
        args = ['/competition/compress_preprocessing_proof.sh']
        return args

    def get_combine_command(self, input_json):
        problem_path = "/logs/processes/input_units_removed.cnf"
        # D.S.: The proof files are now gathered programmatically within the proof_compose script.
        args = ['/competition/proof_compose.sh', problem_path, '/logs/processes/combined.lrat']
        return args

    def get_renumber_command(self, input_json):
        problem_path = input_json.get("problem_path")
        args = ['/competition/proof_renumber.sh'] # the arguments are currently baked into the script
        return args

    def get_check_command(self, input_json):
        problem_path = input_json.get("problem_path")
        args = ['/competition/proof_check.sh', problem_path]
        return args
    
    def get_drat_combine_command(self, input_json):
        args = ['/competition/drat_compose.sh', input_json.get("problem_path")]
        return args

    def get_drat_check_command(self, input_json):
        args = ['/drat-trim', input_json.get("problem_path"), '/logs/processes/final.drat']
        return args

class MallobParser:
    @staticmethod
    def get_result(output_file):
        """
        TODO: Participants should replace this with something more robust for their own solver!
        """
        with open(output_file) as f:
            raw_logs = f.read()
            if "s UNSATISFIABLE" in raw_logs:
                return "UNSATISFIABLE"
            elif "s SATISFIABLE" in raw_logs:
                return "SATISFIABLE"
            elif "result SAT" in raw_logs:
                return "SATISFIABLE"
            elif "result UNSAT" in raw_logs:
                return "UNSATISFIABLE"
            elif "[ERROR]" in raw_logs:
                return "ERROR"
            else:
                return "UNKNOWN"


def main():
    request_directory = sys.argv[1]
    runner = Runner(request_directory)
    runner.logger.info("Hello from /competition/solver")
    
    input_json = runner.get_input_json()
    cmd = runner.get_command(input_json)
    
    runner.logger.info("Running solver ...")
    time_remaining = TOTAL_TIME

    if pipeline_mode == PipelineMode.SINGLE_MACHINE_64HWT_SEQ_PPROD_AND_DRAT:
        unsat_sequence = [lambda: runner.get_compress_preprocessing_proof_command(input_json),
                          lambda: runner.get_combine_command(input_json),
                          lambda: runner.get_renumber_command(input_json), 
                          lambda: runner.get_wc_command(),
                          lambda: runner.get_check_command(input_json),
                          lambda: runner.get_drat_combine_command(input_json),
                          lambda: runner.get_drat_check_command(input_json)]
    else:
        # Pipeline for parallel proof production
        unsat_sequence = [lambda: runner.get_compress_preprocessing_proof_command(input_json),
                          lambda: runner.get_renumber_command(input_json), 
                          lambda: runner.get_wc_command(),
                          lambda: runner.get_check_command(input_json)]

    output = runner.run(cmd, time_remaining)
    result = MallobParser.get_result(output["stdout"])
    return_code = output["return_code"]
    runner.logger.info(f"RESULT: {result}")
    time_remaining = time_remaining - output["time"]
    # proof check
    if (result == "UNSATISFIABLE" and not output["timed_out"]):
        for call in unsat_sequence: 
            cmd = call()
            result = runner.run(cmd, time_remaining, True)
            time_remaining = time_remaining - result["time"]
            return_code = result["return_code"]
            if return_code:
                runner.logger.info(f"Cmd: {cmd} return code was non-zero ({return_code}).  Returning 'ERROR'.")
                result = "ERROR"
                break
            elif output["timed_out"]:
                runner.logger.info(f"Time-out during cmd {cmd}.  Returning 'UNKNOWN'.")
                result = "UNKNOWN"
                break
        if result == "UNSATISFIABLE": 
            runner.logger.info(f"Checker return code was zero.  Check successful!")
                    
    # clean up.
    runner.run(['/competition/cleanup'], True)
    
    solver_output = {
        "return_code": return_code,
        "result": result,
        "artifacts": {
            "stdout_path": output["stdout"],
            "stderr_path": output["stderr"]
        }
    }
    
    with open(os.path.join(request_directory, "solver_out.json"), "w+") as f:
        f.write(json.dumps(solver_output))

if __name__ == "__main__":
    main()
